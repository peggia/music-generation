{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNh0zuqEEOAV"
      },
      "source": [
        "#Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT8wLSNzFnXK",
        "outputId": "850239b9-a4ea-4045-d46e-1e5c671d82e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9xLvy1XW7S0",
        "outputId": "37995390-f1c4-4718-8656-04a52f2faca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MIDI-VAE-NEW\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/MIDI-VAE-NEW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pz5KKp8CFqL3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set the path to your MIDI files\n",
        "midi_folder_path = \"/content/drive/MyDrive/MIDI-VAE-NEW/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMhGZbB-GeR_",
        "outputId": "86f4c4c8-45e1-4397-c940-5478ec63a770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder exists: True\n",
            "Files in folder: ['Pop', 'Jazz']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(f\"Folder exists: {os.path.exists(midi_folder_path)}\")\n",
        "print(f\"Files in folder: {os.listdir(midi_folder_path)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First uninstall existing versions\n",
        "#!pip uninstall -y numpy tensorflow\n"
      ],
      "metadata": {
        "id": "VY0KXfozPz_P"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "sWVQpiUeER82"
      },
      "outputs": [],
      "source": [
        "#!pip install pretty_midi numpy tensorflow matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMtWhh3wJEWv",
        "outputId": "62f4ebf4-6d06-4aa1-a071-ff26b2389f44",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.13.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Requirement already satisfied: pretty_midi==0.2.9 in /usr/local/lib/python3.11/dist-packages (0.2.9)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pretty_midi==0.2.9) (1.23.5)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.11/dist-packages (from pretty_midi==0.2.9) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi==0.2.9) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mido>=1.1.16->pretty_midi==0.2.9) (24.2)\n"
          ]
        }
      ],
      "source": [
        "# Install compatible versions\n",
        "!pip install numpy==1.23.5 tensorflow==2.12.0\n",
        "!pip install pretty_midi==0.2.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJyE1ZWFl6Ei",
        "outputId": "57b14695-2c3e-4c3e-db18-df1c7986c2b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7aAo0GJCkQW"
      },
      "source": [
        "#Commun : MIDI-VAE Class with update Project environemnt (Python 3.11)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# Add to existing imports\n",
        "import gc\n",
        "import warnings\n",
        "\n",
        "\n",
        "# Disable warnings\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)"
      ],
      "metadata": {
        "id": "ghIrKQyCkJ4q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "e18HPUSTCgPn"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class MIDI_VAE(keras.Model):\n",
        "    def __init__(self, sequence_length, input_dim, latent_dim, intermediate_dim):\n",
        "        \"\"\"\n",
        "        Modern implementation of MIDI-VAE using TensorFlow 2.x\n",
        "\n",
        "        Args:\n",
        "            sequence_length: Length of the MIDI sequence\n",
        "            input_dim: Dimension of input features\n",
        "            latent_dim: Dimension of the latent space\n",
        "            intermediate_dim: Dimension of the intermediate layers\n",
        "        \"\"\"\n",
        "        super(MIDI_VAE, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_lstm = layers.LSTM(intermediate_dim,\n",
        "                                      return_sequences=True)\n",
        "        self.encoder_lstm2 = layers.LSTM(intermediate_dim,\n",
        "                                       return_state=True)\n",
        "\n",
        "        # VAE layers\n",
        "        self.dense_mean = layers.Dense(latent_dim)\n",
        "        self.dense_log_var = layers.Dense(latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_initial = layers.Dense(intermediate_dim,\n",
        "                                          activation='relu')\n",
        "        self.decoder_lstm = layers.LSTM(intermediate_dim,\n",
        "                                      return_sequences=True,\n",
        "                                      stateful=False)\n",
        "        self.decoder_dense = layers.Dense(input_dim,\n",
        "                                        activation='sigmoid')\n",
        "\n",
        "        # Store parameters\n",
        "        self.latent_dim = latent_dim\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def encode(self, x):\n",
        "        # Initial LSTM encoding\n",
        "        x = self.encoder_lstm(x)\n",
        "\n",
        "        # Get final states\n",
        "        _, state_h, state_c = self.encoder_lstm2(x)\n",
        "\n",
        "        # Generate latent parameters\n",
        "        z_mean = self.dense_mean(state_h)\n",
        "        z_log_var = self.dense_log_var(state_h)\n",
        "\n",
        "        return z_mean, z_log_var\n",
        "\n",
        "    def reparameterize(self, z_mean, z_log_var):\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        epsilon = tf.random.normal(shape=(batch, self.latent_dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    def decode(self, z):\n",
        "        # Initial state from latent vector\n",
        "        initial_state = self.decoder_initial(z)\n",
        "\n",
        "        # Repeat vector to create sequence\n",
        "        x = tf.repeat(tf.expand_dims(initial_state, 1),\n",
        "                     self.sequence_length, axis=1)\n",
        "\n",
        "        # Decode sequence\n",
        "        x = self.decoder_lstm(x)\n",
        "        reconstruction = self.decoder_dense(x)\n",
        "\n",
        "        return reconstruction\n",
        "\n",
        "    def call(self, x):\n",
        "        # Full forward pass\n",
        "        z_mean, z_log_var = self.encode(x)\n",
        "        z = self.reparameterize(z_mean, z_log_var)\n",
        "        reconstruction = self.decode(z)\n",
        "\n",
        "        # Add KL divergence loss\n",
        "        kl_loss = -0.5 * tf.reduce_mean(\n",
        "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
        "        )\n",
        "        self.add_loss(kl_loss)\n",
        "\n",
        "        return reconstruction\n",
        "\n",
        "# Training utilities\n",
        "@tf.function\n",
        "def train_step(model, x, optimizer):\n",
        "    \"\"\"Single training step\"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        reconstruction = model(x)\n",
        "        # Reconstruction loss (binary crossentropy)\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "            tf.reduce_sum(\n",
        "                keras.losses.binary_crossentropy(x, reconstruction),\n",
        "                #axis=[1, 2]\n",
        "                axis=[1] # problematic line corrected\n",
        "            )\n",
        "        )\n",
        "        # Total loss (including KL divergence added in model.call)\n",
        "        total_loss = reconstruction_loss + tf.reduce_sum(model.losses)\n",
        "\n",
        "    # Compute and apply gradients\n",
        "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return total_loss, reconstruction_loss\n",
        "\n",
        "# Example usage\n",
        "def create_and_compile_model(sequence_length=32,\n",
        "                           input_dim=128,\n",
        "                           latent_dim=256,\n",
        "                           intermediate_dim=512):\n",
        "    \"\"\"Create and compile the MIDI-VAE model\"\"\"\n",
        "    model = MIDI_VAE(sequence_length, input_dim, latent_dim, intermediate_dim)\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "    # Build model\n",
        "    dummy_input = tf.zeros((1, sequence_length, input_dim))\n",
        "    _ = model(dummy_input)\n",
        "\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP7kUyJOG1tC"
      },
      "source": [
        "#MIDI Preprocessing Test Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGpF1Y2BJzaA",
        "outputId": "e04ca60e-2745-40af-c4cf-b4a2342f17c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting MIDI preprocessing test...\n",
            "=== MIDI Preprocessing Test ===\n",
            "\n",
            "1. Checking MIDI files...\n",
            "✓ Found 99 MIDI files\n",
            "\n",
            "Sample files:\n",
            "- Pop/Bobby_Vinton_-_Sealed_With_a_Kiss.mid\n",
            "- Pop/Boyzone_-_Fathers_And_Sons.mid\n",
            "- Pop/Bonnie_Tyler_-_Total_Eclipse_of_the_Heart.mid\n",
            "\n",
            "2. Initializing preprocessor...\n",
            "\n",
            "3. Testing file processing...\n",
            "Testing file: Bobby_Vinton_-_Sealed_With_a_Kiss.mid\n",
            "✓ Successfully processed:\n",
            "- Duration: 146.51 sec\n",
            "- Extracted 1141 sequences\n",
            "- Sequence shape: (32, 89)\n",
            "=== MIDI Preprocessing Test ===\n",
            "\n",
            "1. Checking MIDI files...\n",
            "✓ Found 99 MIDI files\n",
            "\n",
            "Sample files:\n",
            "- Pop/Bobby_Vinton_-_Sealed_With_a_Kiss.mid\n",
            "- Pop/Boyzone_-_Fathers_And_Sons.mid\n",
            "- Pop/Bonnie_Tyler_-_Total_Eclipse_of_the_Heart.mid\n",
            "\n",
            "2. Initializing preprocessor...\n",
            "\n",
            "3. Testing file processing...\n",
            "Testing file: Bobby_Vinton_-_Sealed_With_a_Kiss.mid\n",
            "✓ Successfully processed:\n",
            "- Duration: 146.51 sec\n",
            "- Extracted 1141 sequences\n",
            "- Sequence shape: (32, 89)\n"
          ]
        }
      ],
      "source": [
        "import pretty_midi\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "class MIDIPreprocessor:\n",
        "    def __init__(self, sequence_length=32,\n",
        "                 min_pitch=21, max_pitch=109,\n",
        "                 time_step=0.125):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.min_pitch = min_pitch\n",
        "        self.max_pitch = max_pitch\n",
        "        self.time_step = time_step\n",
        "        self.n_pitches = max_pitch - min_pitch + 1\n",
        "\n",
        "    def load_midi_file(self, file_path):\n",
        "        try:\n",
        "            # Use explicit integer types\n",
        "            pretty_midi.pretty_midi.MAX_TICK = np.int64(1e7)\n",
        "            midi_data = pretty_midi.PrettyMIDI(file_path)\n",
        "            return midi_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def midi_to_piano_roll(self, midi_data):\n",
        "        try:\n",
        "            # Get piano roll with explicit sampling rate\n",
        "            fs = int(1/self.time_step)\n",
        "            piano_roll = midi_data.get_piano_roll(fs=fs)\n",
        "\n",
        "            # Ensure we have enough pitches\n",
        "            if piano_roll.shape[0] < self.max_pitch + 1:\n",
        "                pad_size = self.max_pitch + 1 - piano_roll.shape[0]\n",
        "                piano_roll = np.pad(piano_roll, ((0, pad_size), (0, 0)))\n",
        "\n",
        "            # Crop to our pitch range\n",
        "            piano_roll = piano_roll[self.min_pitch:self.max_pitch + 1]\n",
        "\n",
        "            # Convert to binary (note on/off)\n",
        "            piano_roll = (piano_roll > 0).astype(np.float32)\n",
        "\n",
        "            return piano_roll.T\n",
        "        except Exception as e:\n",
        "            print(f\"Error in piano roll conversion: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def extract_sequences(self, piano_roll):\n",
        "        if piano_roll is None:\n",
        "            return np.array([])\n",
        "\n",
        "        sequences = []\n",
        "        for i in range(0, len(piano_roll) - self.sequence_length + 1):\n",
        "            sequence = piano_roll[i:i + self.sequence_length]\n",
        "            if np.any(sequence):\n",
        "                sequences.append(sequence)\n",
        "\n",
        "        if not sequences:\n",
        "            return np.array([])\n",
        "\n",
        "        return np.array(sequences)\n",
        "\n",
        "    def create_tf_dataset(self, midi_files, batch_size=32, shuffle=True):\n",
        "        all_sequences = []\n",
        "        processed_files = 0\n",
        "\n",
        "        for file_path in midi_files:\n",
        "            try:\n",
        "                midi_data = self.load_midi_file(file_path)\n",
        "                if midi_data is None:\n",
        "                    continue\n",
        "\n",
        "                piano_roll = self.midi_to_piano_roll(midi_data)\n",
        "                if piano_roll is None:\n",
        "                    continue\n",
        "\n",
        "                sequences = self.extract_sequences(piano_roll)\n",
        "                if len(sequences) > 0:\n",
        "                    all_sequences.append(sequences)\n",
        "                    processed_files += 1\n",
        "\n",
        "                if processed_files % 10 == 0:\n",
        "                    print(f\"Processed {processed_files} files...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_path}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if not all_sequences:\n",
        "            raise ValueError(\"No valid sequences were extracted from the MIDI files\")\n",
        "\n",
        "        all_sequences = np.concatenate(all_sequences, axis=0)\n",
        "        print(f\"Created dataset with {len(all_sequences)} sequences\")\n",
        "\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(all_sequences)\n",
        "        if shuffle:\n",
        "            dataset = dataset.shuffle(10000)\n",
        "\n",
        "        dataset = dataset.batch(batch_size)\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        return dataset\n",
        "#-----\n",
        "def test_midi_preprocessing(midi_folder_path, check_subfolders=True):\n",
        "    \"\"\"\n",
        "    Test MIDI preprocessing pipeline with style folder support\n",
        "\n",
        "    Args:\n",
        "        midi_folder_path: Root folder containing MIDIs\n",
        "        check_subfolders: If True, looks in style subfolders\n",
        "    \"\"\"\n",
        "    print(\"=== MIDI Preprocessing Test ===\")\n",
        "\n",
        "    # 1. File Discovery\n",
        "    print(\"\\n1. Checking MIDI files...\")\n",
        "    if check_subfolders:\n",
        "        # Look in style subfolders\n",
        "        style_folders = [f for f in os.listdir(midi_folder_path)\n",
        "                        if os.path.isdir(os.path.join(midi_folder_path, f))]\n",
        "        midi_files = []\n",
        "        for style in style_folders:\n",
        "            style_path = os.path.join(midi_folder_path, style)\n",
        "            midi_files.extend(glob.glob(os.path.join(style_path, \"*.mid\")))\n",
        "            midi_files.extend(glob.glob(os.path.join(style_path, \"*.midi\")))\n",
        "    else:\n",
        "        # Original behavior (root folder only)\n",
        "        midi_files = glob.glob(os.path.join(midi_folder_path, \"*.mid\"))\n",
        "        midi_files.extend(glob.glob(os.path.join(midi_folder_path, \"*.midi\")))\n",
        "\n",
        "    if not midi_files:\n",
        "        print(\"❌ No MIDI files found!\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"✓ Found {len(midi_files)} MIDI files\")\n",
        "    print(\"\\nSample files:\")\n",
        "    for f in midi_files[:3]:\n",
        "        print(f\"- {os.path.relpath(f, midi_folder_path)}\")\n",
        "\n",
        "    # 2. Initialize Preprocessor\n",
        "    print(\"\\n2. Initializing preprocessor...\")\n",
        "    preprocessor = MIDIPreprocessor(\n",
        "        sequence_length=32,\n",
        "        min_pitch=21,\n",
        "        max_pitch=109,\n",
        "        time_step=0.125\n",
        "    )\n",
        "\n",
        "    # 3. Test Processing\n",
        "    print(\"\\n3. Testing file processing...\")\n",
        "    test_file = midi_files[0]  # Test first file\n",
        "    print(f\"Testing file: {os.path.basename(test_file)}\")\n",
        "\n",
        "    try:\n",
        "        midi_data = preprocessor.load_midi_file(test_file)\n",
        "        if midi_data is None:\n",
        "            print(\"❌ Failed to load test file\")\n",
        "            return None, None\n",
        "\n",
        "        piano_roll = preprocessor.midi_to_piano_roll(midi_data)\n",
        "        if piano_roll is None:\n",
        "            print(\"❌ Failed to create piano roll\")\n",
        "            return None, None\n",
        "\n",
        "        sequences = preprocessor.extract_sequences(piano_roll)\n",
        "        if len(sequences) == 0:\n",
        "            print(\"❌ No sequences extracted\")\n",
        "            return None, None\n",
        "\n",
        "        print(\"✓ Successfully processed:\")\n",
        "        print(f\"- Duration: {midi_data.get_end_time():.2f} sec\")\n",
        "        print(f\"- Extracted {len(sequences)} sequences\")\n",
        "        print(f\"- Sequence shape: {sequences[0].shape}\")\n",
        "\n",
        "        return sequences, preprocessor\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {test_file}: {str(e)}\")\n",
        "        return None, None\n",
        "#----\n",
        "\n",
        "# Run the test\n",
        "print(\"Starting MIDI preprocessing test...\")\n",
        "# Test with style subfolders\n",
        "sequences, preprocessor = test_midi_preprocessing(\n",
        "    midi_folder_path=\"/content/drive/MyDrive/MIDI-VAE-NEW/data\",\n",
        "    check_subfolders=True\n",
        ")\n",
        "\n",
        "dataset, preprocessor = test_midi_preprocessing(midi_folder_path)\n",
        "\n",
        "# if dataset is not None:\n",
        "#     print(\"\\nDataset creation successful!\")\n",
        "#     # Show first batch info\n",
        "#     for batch in dataset.take(1):\n",
        "#         print(f\"Batch shape: {batch.shape}\")\n",
        "#         print(f\"Batch min value: {tf.reduce_min(batch)}\")\n",
        "#         print(f\"Batch max value: {tf.reduce_max(batch)}\")\n",
        "# else:\n",
        "#     print(\"\\nFailed to create dataset. Please check the errors above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywqqnwjvn7WM"
      },
      "source": [
        "# Convert midi to wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "_Ohf1ngJxehn"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "# # Convert MIDI to WAV using fluidsynth\n",
        "# !apt-get install fluidsynth\n",
        "# # !wget https://download.sf2tool.com/GeneralUser_GS_1.471.zip\n",
        "# # !unzip GeneralUser_GS_1.471.zip\n",
        "# !wget https://www.philscomputerlab.com/uploads/3/7/2/3/37231621/weedsgm3.sf2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#V1 only"
      ],
      "metadata": {
        "id": "jRvJs_Nxzj2C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E3lipS7B--bU",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# midi_file_path = \"/content/drive/MyDrive/MIDI-VAE-NEW/generated/generated_music.mid\"\n",
        "# # Convert MIDI to WAV\n",
        "# wav_file_path = \"/content/drive/MyDrive/MIDI-VAE-NEW/generated/generated_music.wav\"\n",
        "# !fluidsynth -ni weedsgm3.sf2 {midi_file_path} -F {wav_file_path} -r 44100\n",
        "\n",
        "# # The rate argument is added to Audio to specify the sample rate.\n",
        "# Audio(wav_file_path, rate=44100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "F08Yh154TA4S"
      },
      "outputs": [],
      "source": [
        "# midi_file_path = \"/content/drive/MyDrive/MIDI-VAE-NEW/generated/interpolated_music.mid\"\n",
        "# # Convert MIDI to WAV\n",
        "# wav_file_path = \"/content/drive/MyDrive/MIDI-VAE-NEW/generated/interpolated_music.wav\"\n",
        "# !fluidsynth -ni weedsgm3.sf2 {midi_file_path} -F {wav_file_path} -r 44100\n",
        "\n",
        "# # The rate argument is added to Audio to specify the sample rate.\n",
        "# Audio(wav_file_path, rate=44100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sYAAw42F-Jzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Style Conditioned MIDI-VAE"
      ],
      "metadata": {
        "id": "5StCyY_0Zlxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class definition"
      ],
      "metadata": {
        "id": "VqyKu_hCdccX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Remove mixed precision code completely (no imports or policy settings)\n",
        "\n",
        "# 2. Use this simplified StyleConditionedMIDI_VAE class\n",
        "class StyleConditionedMIDI_VAE(keras.Model):\n",
        "    def __init__(self, sequence_length, input_dim, latent_dim, intermediate_dim, num_styles):\n",
        "        super(StyleConditionedMIDI_VAE, self).__init__()\n",
        "\n",
        "        # Style embedding\n",
        "        self.style_embedding_dim = 32\n",
        "        self.style_embedding = layers.Embedding(num_styles, self.style_embedding_dim)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_lstm = layers.LSTM(intermediate_dim, return_sequences=True)\n",
        "        self.encoder_lstm2 = layers.LSTM(intermediate_dim, return_state=True)\n",
        "\n",
        "        # VAE layers\n",
        "        self.dense_mean = layers.Dense(latent_dim)\n",
        "        self.dense_log_var = layers.Dense(latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_initial = layers.Dense(intermediate_dim, activation='relu')\n",
        "        self.decoder_lstm = layers.LSTM(intermediate_dim, return_sequences=True)\n",
        "        self.decoder_dense = layers.Dense(input_dim, activation='sigmoid')\n",
        "\n",
        "        # Store parameters\n",
        "        self.latent_dim = latent_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.num_styles = num_styles\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, style_ids = inputs\n",
        "        x = tf.cast(x, tf.float32)  # Ensure float32\n",
        "\n",
        "        # Encode\n",
        "        style_embed = self.style_embedding(style_ids)\n",
        "        x = self.encoder_lstm(x)\n",
        "        _, state_h, _ = self.encoder_lstm2(x)\n",
        "        combined = tf.concat([state_h, style_embed], axis=-1)\n",
        "        z_mean = self.dense_mean(combined)\n",
        "        z_log_var = self.dense_log_var(combined)\n",
        "\n",
        "        # Reparameterize\n",
        "        z = z_mean + tf.exp(0.5 * z_log_var) * tf.random.normal(shape=tf.shape(z_mean))\n",
        "\n",
        "        # Decode\n",
        "        style_embed = self.style_embedding(style_ids)\n",
        "        z_combined = tf.concat([z, style_embed], axis=-1)\n",
        "        initial_state = self.decoder_initial(z_combined)\n",
        "        x = tf.repeat(tf.expand_dims(initial_state, 1), self.sequence_length, axis=1)\n",
        "        x = self.decoder_lstm(x)\n",
        "        reconstruction = self.decoder_dense(x)\n",
        "\n",
        "        # KL loss\n",
        "        kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "        self.add_loss(kl_loss)\n",
        "\n",
        "        return reconstruction\n",
        "\n",
        "# 3. Use this simplified dataset creation\n",
        "def create_style_conditioned_dataset(datasets, style_ids, batch_size=32):\n",
        "    \"\"\"Create dataset with float32 dtype and proper batching\"\"\"\n",
        "    # Concatenate all data\n",
        "    all_data = np.concatenate([datasets[style] for style in datasets], axis=0)\n",
        "    all_styles = np.concatenate([style_ids[style] for style in style_ids], axis=0)\n",
        "\n",
        "    # Create dataset pipeline\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((all_data, all_styles))\n",
        "    dataset = dataset.shuffle(10000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# 4. Simplified training approach\n",
        "def train_model():\n",
        "    # 1. Load data\n",
        "    _, preprocessor = test_midi_preprocessing(midi_folder_path, check_subfolders=True)\n",
        "    datasets, style_ids = load_style_data(midi_folder_path, {'Jazz':0, 'Mozart':1}, preprocessor)\n",
        "\n",
        "    # 2. Create dataset with smaller batch size\n",
        "    dataset = create_style_conditioned_dataset(datasets, style_ids, batch_size=8)  # Reduced batch\n",
        "\n",
        "    # 3. Create smaller model\n",
        "    model, optimizer = create_and_compile_model_style_conditioned(\n",
        "        sequence_length=32,\n",
        "        input_dim=89,  # From your data\n",
        "        latent_dim=64,  # Reduced\n",
        "        intermediate_dim=128,  # Reduced\n",
        "        num_styles=2\n",
        "    )\n",
        "\n",
        "    # 4. Train with memory monitoring\n",
        "    for epoch in range(5):\n",
        "        print(f\"Epoch {epoch+1}\")\n",
        "        for x, y in dataset:\n",
        "            with tf.GradientTape() as tape:\n",
        "                recon = model((x, y))\n",
        "                loss = tf.reduce_mean(keras.losses.binary_crossentropy(x, recon))\n",
        "                loss += sum(model.losses)\n",
        "\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "            print(f\"Batch loss: {loss.numpy():.4f}\")\n",
        "\n",
        "        # Clear memory\n",
        "        gc.collect()\n",
        "        tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "pE8Zdk26HBaa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Style conditioned VAE Trainer"
      ],
      "metadata": {
        "id": "qmr3zTIPhOs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleConditionedVAETrainer:\n",
        "    def __init__(self, model, checkpoint_dir, optimizer=None):\n",
        "        self.model = model\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.optimizer = optimizer or tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "        # Setup checkpoint manager\n",
        "        self.checkpoint = tf.train.Checkpoint(\n",
        "            model=self.model,\n",
        "            optimizer=self.optimizer\n",
        "        )\n",
        "        self.manager = tf.train.CheckpointManager(\n",
        "            self.checkpoint,\n",
        "            checkpoint_dir,\n",
        "            max_to_keep=3\n",
        "        )\n",
        "\n",
        "        # Restore if available\n",
        "        if self.manager.latest_checkpoint:\n",
        "            self.checkpoint.restore(self.manager.latest_checkpoint)\n",
        "            print(f\"Restored from {self.manager.latest_checkpoint}\")\n",
        "        else:\n",
        "            print(\"Initializing from scratch\")\n",
        "\n",
        "        # Metrics\n",
        "        self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "        self.reconstruction_loss = tf.keras.metrics.Mean(name='reconstruction_loss')\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, x, style_ids):\n",
        "        \"\"\"Corrected training step - now instance method\"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            reconstruction = self.model((x, style_ids))\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.keras.losses.binary_crossentropy(x, reconstruction)\n",
        "            )\n",
        "            total_loss = reconstruction_loss + tf.reduce_sum(self.model.losses)\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "\n",
        "        self.train_loss.update_state(total_loss)\n",
        "        self.reconstruction_loss.update_state(reconstruction_loss)\n",
        "\n",
        "        return total_loss, reconstruction_loss\n",
        "\n",
        "    def train(self, dataset, epochs, save_freq='epoch'):\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "            # Reset metrics\n",
        "            self.train_loss.reset_states()\n",
        "            self.reconstruction_loss.reset_states()\n",
        "\n",
        "            # Training loop\n",
        "            for step, (x, style_ids) in enumerate(dataset):\n",
        "                loss, rec_loss = self.train_step(x, style_ids)\n",
        "\n",
        "                if step % 500 == 0:\n",
        "                    print(f\"Step {step}: Loss={loss:.4f}, Recon={rec_loss:.4f}\")\n",
        "\n",
        "            # End of epoch\n",
        "            print(f\"Epoch {epoch+1} - Loss: {self.train_loss.result():.4f}\")\n",
        "\n",
        "            if save_freq == 'epoch':\n",
        "                save_path = self.manager.save()\n",
        "                print(f\"Saved checkpoint: {save_path}\")\n",
        "\n",
        "            # Memory management\n",
        "            gc.collect()\n",
        "            tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "HS7y9Gi-hRJe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create style-specific datasets"
      ],
      "metadata": {
        "id": "FpaapcbNaMZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map style names to IDs\n",
        "style_map = {'Jazz': 0, 'Pop': 1} # Tu peux ajouter 'Classical': 2 si nécessaire\n",
        "\n",
        "# Fonction complète pour charger et prétraiter les données MIDI avec style IDs\n",
        "def load_style_data(midi_folder_path, style_map, preprocessor=None, max_files=50):\n",
        "    if preprocessor is None:\n",
        "        preprocessor = MIDIPreprocessor()  # Create with default params if none provided\n",
        "    datasets = {}\n",
        "    style_ids = {}\n",
        "\n",
        "    for style_name, style_id in style_map.items():\n",
        "        style_path = os.path.join(midi_folder_path, style_name)\n",
        "        if not os.path.exists(style_path):\n",
        "            continue\n",
        "\n",
        "        midi_files = [f for f in os.listdir(style_path) if f.endswith(('.mid','.midi'))][:max_files]\n",
        "        all_sequences = []\n",
        "\n",
        "        for midi_file in tqdm(midi_files, desc=f\"Processing {style_name}\"):\n",
        "            try:\n",
        "                file_path = os.path.join(style_path, midi_file)\n",
        "                midi_data = preprocessor.load_midi_file(file_path)\n",
        "                if midi_data is None:\n",
        "                    continue\n",
        "\n",
        "                piano_roll = preprocessor.midi_to_piano_roll(midi_data)\n",
        "                sequences = preprocessor.extract_sequences(piano_roll)\n",
        "                if len(sequences) > 0:\n",
        "                    all_sequences.append(sequences)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nSkipping {midi_file}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if all_sequences:\n",
        "            datasets[style_name] = np.concatenate(all_sequences)\n",
        "            style_ids[style_name] = np.full(len(datasets[style_name]), style_id)\n",
        "\n",
        "    return datasets, style_ids\n",
        "\n",
        "\n",
        "def create_style_conditioned_dataset(datasets, style_ids, batch_size=32):\n",
        "    \"\"\"Simplified dataset creation without mixed precision\"\"\"\n",
        "    all_data = []\n",
        "    all_style_ids = []\n",
        "\n",
        "    for style_name in datasets.keys():\n",
        "        all_data.append(datasets[style_name])\n",
        "        all_style_ids.append(style_ids[style_name])\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_data = np.concatenate(all_data, axis=0).astype(np.float32)  # Use float32\n",
        "    all_style_ids = np.concatenate(all_style_ids, axis=0).astype(np.int32)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((all_data, all_style_ids))\n",
        "    dataset = dataset.shuffle(buffer_size=10000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Verify\n",
        "    for batch in dataset.take(1):\n",
        "        print(\"\\nDataset sample:\")\n",
        "        print(f\"Data shape: {batch[0].shape}, dtype: {batch[0].dtype}\")\n",
        "        print(f\"Style shape: {batch[1].shape}, dtype: {batch[1].dtype}\")\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "jSt2nAQvaMz9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Createand train the style conditioned model"
      ],
      "metadata": {
        "id": "Z2HgfqUxd68e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_compile_model_style_conditioned(sequence_length=32,\n",
        "                                            input_dim=128,\n",
        "                                            latent_dim=256,\n",
        "                                            intermediate_dim=512,\n",
        "                                            num_styles=2):\n",
        "    \"\"\"Create and compile the style-conditioned MIDI-VAE model\"\"\"\n",
        "    model = StyleConditionedMIDI_VAE(\n",
        "        sequence_length,\n",
        "        input_dim,\n",
        "        latent_dim,\n",
        "        intermediate_dim,\n",
        "        num_styles\n",
        "    )\n",
        "\n",
        "    # Test model build\n",
        "    try:\n",
        "        dummy_input = (tf.zeros((1, sequence_length, input_dim), dtype=tf.float32),\n",
        "                      tf.zeros((1,), dtype=tf.int32))\n",
        "        _ = model(dummy_input)\n",
        "        print(\"✓ Model successfully built with:\")\n",
        "        print(f\"- Input shape: {dummy_input[0].shape}\")\n",
        "        print(f\"- Style shape: {dummy_input[1].shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Model building failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    return model, optimizer"
      ],
      "metadata": {
        "id": "ERxMurkhd5vG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Exécution complète du style conditioning**"
      ],
      "metadata": {
        "id": "aLcQMs3lCjI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rI8nCkCiNYkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  import gc"
      ],
      "metadata": {
        "id": "_ruQpbePisoq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_style_conditioning():\n",
        "    # 1. Define paths and style mapping\n",
        "    midi_folder_path = \"/content/drive/MyDrive/MIDI-VAE-NEW/data\"\n",
        "    style_map = {'Jazz': 0, 'Pop': 1}\n",
        "\n",
        "    # 2. Load and preprocess data\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    datasets, style_ids = load_style_data(midi_folder_path, style_map, preprocessor)\n",
        "\n",
        "    # If no datasets found, use dummy data\n",
        "    if not datasets:\n",
        "        print(\"No datasets found. Using dummy data for testing...\")\n",
        "        sequence_length = 32\n",
        "        input_dim = preprocessor.n_pitches if hasattr(preprocessor, 'n_pitches') else 88\n",
        "        datasets = {\n",
        "            'Jazz': np.random.rand(100, sequence_length, input_dim).astype(np.float32),\n",
        "            'Pop': np.random.rand(100, sequence_length, input_dim).astype(np.float32)\n",
        "        }\n",
        "        style_ids = {\n",
        "            'Jazz': np.full(100, 0),\n",
        "            'Pop': np.full(100, 1)\n",
        "        }\n",
        "\n",
        "    # 3. Create TensorFlow dataset\n",
        "    print(\"Creating TensorFlow dataset...\")\n",
        "    combined_dataset = create_style_conditioned_dataset(datasets, style_ids)\n",
        "\n",
        "    # 4. Define model parameters\n",
        "    first_batch = next(iter(combined_dataset))\n",
        "    sequence_length = first_batch[0].shape[1]\n",
        "    input_dim = first_batch[0].shape[2]\n",
        "    latent_dim = 64 #256\n",
        "    intermediate_dim = 128 # 512\n",
        "    num_styles = len(style_map)\n",
        "\n",
        "    print(f\"\\nModel parameters:\")\n",
        "    print(f\"- Sequence length: {sequence_length}\")\n",
        "    print(f\"- Input dimension: {input_dim}\")\n",
        "    print(f\"- Latent dimension: {latent_dim}\")\n",
        "    print(f\"- Number of styles: {num_styles}\")\n",
        "\n",
        "    # 5. Create and compile model\n",
        "    print(\"\\nCreating and compiling model...\")\n",
        "    model_style, optimizer_style = create_and_compile_model_style_conditioned(\n",
        "        sequence_length=sequence_length,\n",
        "        input_dim=input_dim,\n",
        "        latent_dim=latent_dim,\n",
        "        intermediate_dim=intermediate_dim,\n",
        "        num_styles=num_styles\n",
        "    )\n",
        "\n",
        "    # 6. Setup training\n",
        "    checkpoint_dir = \"/content/drive/MyDrive/MIDI-VAE-NEW/checkpoints_style\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    trainer = StyleConditionedVAETrainer(model_style, checkpoint_dir, optimizer_style)\n",
        "    trainer.train(combined_dataset, epochs=3,save_freq=1000 )\n",
        "\n",
        "    return model_style, datasets"
      ],
      "metadata": {
        "id": "vLbAqWy-Cm9z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this before calling run_style_conditioning()\n",
        "\n",
        "\n",
        "# 1. Initialize\n",
        "\n",
        "_, preprocessor = test_midi_preprocessing(midi_folder_path, check_subfolders=True)\n",
        "# 2. Run with checks\n",
        "try:\n",
        "    model_style, style_datasets = run_style_conditioning()\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Falling back to minimal working example...\")\n",
        "    # Create tiny valid model\n",
        "    model_style = StyleConditionedMIDI_VAE(32, 88, 64, 128, 2)\n",
        "    style_datasets = {}"
      ],
      "metadata": {
        "id": "x5mgBwuxDDhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf624d1-9916-45be-e69c-946b6efe6aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== MIDI Preprocessing Test ===\n",
            "\n",
            "1. Checking MIDI files...\n",
            "✓ Found 99 MIDI files\n",
            "\n",
            "Sample files:\n",
            "- Pop/Bobby_Vinton_-_Sealed_With_a_Kiss.mid\n",
            "- Pop/Boyzone_-_Fathers_And_Sons.mid\n",
            "- Pop/Bonnie_Tyler_-_Total_Eclipse_of_the_Heart.mid\n",
            "\n",
            "2. Initializing preprocessor...\n",
            "\n",
            "3. Testing file processing...\n",
            "Testing file: Bobby_Vinton_-_Sealed_With_a_Kiss.mid\n",
            "✓ Successfully processed:\n",
            "- Duration: 146.51 sec\n",
            "- Extracted 1141 sequences\n",
            "- Sequence shape: (32, 89)\n",
            "Loading and preprocessing data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Jazz:  76%|███████▌  | 38/50 [00:13<00:04,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading /content/drive/MyDrive/MIDI-VAE-NEW/data/Jazz/a_taste_of_honey_jc2.mid: MIDI file has a largest tick of 11839910, it is likely corrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Jazz: 100%|██████████| 50/50 [00:19<00:00,  2.62it/s]\n",
            "Processing Pop: 100%|██████████| 49/49 [00:10<00:00,  4.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating TensorFlow dataset...\n",
            "\n",
            "Dataset sample:\n",
            "Data shape: (32, 32, 89), dtype: <dtype: 'float32'>\n",
            "Style shape: (32,), dtype: <dtype: 'int32'>\n",
            "\n",
            "Model parameters:\n",
            "- Sequence length: 32\n",
            "- Input dimension: 89\n",
            "- Latent dimension: 256\n",
            "- Number of styles: 2\n",
            "\n",
            "Creating and compiling model...\n",
            "✓ Model successfully built with:\n",
            "- Input shape: (1, 32, 89)\n",
            "- Style shape: (1,)\n",
            "\n",
            "Starting training...\n",
            "Initializing from scratch\n",
            "\n",
            "Epoch 1/3\n",
            "Step 0: Loss=0.6999, Recon=0.6989\n",
            "Step 100: Loss=0.1582, Recon=0.1581\n",
            "Step 200: Loss=0.1581, Recon=0.1580\n",
            "Step 300: Loss=0.1775, Recon=0.1775\n",
            "Step 400: Loss=0.1630, Recon=0.1630\n",
            "Step 500: Loss=0.1566, Recon=0.1566\n",
            "Step 600: Loss=0.1670, Recon=0.1670\n",
            "Step 700: Loss=0.2084, Recon=0.2084\n",
            "Step 800: Loss=0.2062, Recon=0.2062\n",
            "Step 900: Loss=0.2134, Recon=0.2134\n",
            "Step 1000: Loss=0.2068, Recon=0.2067\n",
            "Step 1100: Loss=0.1937, Recon=0.1937\n",
            "Step 1200: Loss=0.1708, Recon=0.1706\n",
            "Step 1300: Loss=0.2128, Recon=0.2089\n",
            "Step 1400: Loss=0.1772, Recon=0.1757\n",
            "Step 1500: Loss=0.1715, Recon=0.1680\n",
            "Step 1600: Loss=0.1861, Recon=0.1843\n",
            "Step 1700: Loss=0.1904, Recon=0.1879\n",
            "Step 1800: Loss=0.1954, Recon=0.1927\n",
            "Step 1900: Loss=0.1952, Recon=0.1926\n",
            "Step 2000: Loss=0.1966, Recon=0.1939\n",
            "Step 2100: Loss=0.2003, Recon=0.1952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "dlnsZ50yNaJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generation de musique"
      ],
      "metadata": {
        "id": "VnljJBfEYaW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_music_from_model(model, style_id, output_path='generated_song.mid', temperature=1.0):\n",
        "    import pretty_midi\n",
        "\n",
        "    print(f\"Generating music in style ID: {style_id}...\")\n",
        "\n",
        "    # 1. Générer une séquence\n",
        "    generated = model.generate_with_style(style_id=style_id, num_samples=1, temperature=temperature)\n",
        "    generated = generated.numpy()[0]  # retirer le batch\n",
        "\n",
        "    # 2. Convertir le piano roll en PrettyMIDI\n",
        "    pm = pretty_midi.PrettyMIDI()\n",
        "    instrument = pretty_midi.Instrument(program=0)  # Piano\n",
        "\n",
        "    fs = int(1 / 0.125)  # 8 temps par beat\n",
        "    threshold = 0.3  # Binarisation du piano roll\n",
        "\n",
        "    for pitch in range(generated.shape[1]):\n",
        "        active = False\n",
        "        note_on = 0\n",
        "        for t in range(generated.shape[0]):\n",
        "            if generated[t, pitch] > threshold and not active:\n",
        "                active = True\n",
        "                note_on = t\n",
        "            elif generated[t, pitch] <= threshold and active:\n",
        "                note_off = t\n",
        "                start = note_on / fs\n",
        "                end = note_off / fs\n",
        "                note = pretty_midi.Note(\n",
        "                    velocity=100,\n",
        "                    pitch=pitch,\n",
        "                    start=start,\n",
        "                    end=end\n",
        "                )\n",
        "                instrument.notes.append(note)\n",
        "                active = False\n",
        "\n",
        "    pm.instruments.append(instrument)\n",
        "\n",
        "    # 3. Sauvegarder en MIDI\n",
        "    pm.write(output_path)\n",
        "    print(f\"Generated music saved at: {output_path}\")\n"
      ],
      "metadata": {
        "id": "m3_S71luYZXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_style, style_datasets = run_style_conditioning()\n",
        "\n",
        "# Génère un morceau en style Jazz (ID = 0)\n",
        "generate_music_from_model(model_style, style_id=0, output_path=\"jazz_generated.mid\")\n",
        "\n",
        "# Génère un morceau en style Pop (ID = 1)\n",
        "generate_music_from_model(model_style, style_id=1, output_path=\"pop_generated.mid\")\n"
      ],
      "metadata": {
        "id": "CSawHU-4YlQk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}